gradient descent알고리즘을 사용하면 learning rate이 크게 되면
cost의 경우 너무 많이 뛰어넘어서 제대로 된 값을 찾지 못하고 벗어나 버리기도 한다.
그럴 경우 무한대로 넘어가서 제대로 된 값을 구하지 못하게 된다.
-=-=> Overshooting이라한다.

반대로 lerning rate를 매우 작게 한다면 많은 학습을 해도 cost가 크게 줄어들지 않아서
중간에 멈춰버리거나, 매우 오래 걸리게 된다.